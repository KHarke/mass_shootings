---
title:  "Cluster Analysis of Mass Shooting Locations"
author: "Tochi Agbara, Karen Harker, Kevin Brown"
editor: visual
---

# Business Problem

We will work with The Violence Project's Mass Shooter Database to better understand the underlying factors, patterns, and consequences associated with public mass shootings. This could aid in the development of preventive strategies, interventions, and support for survivors.  The data will be examined at three levels: the perpetrator, the victims, and the locations or communities.  The desired outcomes will be models of typologies of each of these levels that could be used to generate new research questions, risk assessment models, prevention interventions, and trauma interventions for survivors and witnesses.   

**Methods**

We would typically use machine learning algorithms, such as classification models, or bootstrapping to build more data resample with replacement. These will help to build a predictive model for identifying individuals at risk of perpetrating shooting incidents or being victims, or locations at risk. We could also do a correlation test of mass shooters in Texas to ascertain the gun control level. We are focusing on using unsupervised learning by clustering analysis using age, criminal background, ethnicity, education, etc. to help the policymakers to minimize casualties. Finally, suggestions for future research on the relationship between homicide and the Internet will be considered (Patricia, 2021). 

## Read the data / Import the Data

The Violence Project, a non-profit, nonpartisan research organization dedicated to ... , maintains the Mass Shooter Database (MSDB) (<https://www.theviolenceproject.org/mass-shooter-database/>). The full dataset is available upon request and is provided in an Excel workbook of multiple worksheets, including a codebook or data dictionary. The unit of analysis of the data is the individual mass shooter identified by a case number. There are sheets that contain data about individual victims, the weapons used in each shooting or brought to the scene, about the community at or near the time of the event, and national-level population trend data. The "Full Database" sheet summarizes these data at the shooter level, making it the prime source to use, at least initially.

The Full Database worksheet was extracted from the Excel file and saved as a separate CSV file. There were textual inconsistencies within the database as originally provided, making it difficult to run certain functions. Some modifications were made using Excel, including:

-   Replaced "Case \#" header with "CaseNum" - this makes things easier for matching and referencing the case numbers.

<!-- -->

-   Removed non-standard text from field names, including spaces, dashes, slashes, and parentheses. For some, I replaced with underscores, but not for spaces.

<!-- -->

-   Made the Zipcode field text and added a leading zero to those that needed it (Excel converted these to integers).

<!-- -->

-   Reconciled inconsistent single- and double-quotes in the long-text fields. These were causing the data to be shifted to a new row.

<!-- -->

-   Replaced case \# "145,146" with two rows for each case: 145, 146, duplicating the data. These apparently were one event but two shooters. But there isn't much data about them, so we'll probably drop these cases, anyhow. But at least this way, the file is readable.

To help with the analysis, the field names were modified to include an abbreviation of the category of the variable (based on the codebook) as a prefix. For example, "Age" was changed to "OB_Age", with "OB" abbreviation of "Offender Background" category. Linking these field names to the codebook will also help with identifying factors and other data management tasks. In order to avoid confusion, this version was saved as a separate CSV file, Full_database_VarCatHeaders.csv.

```{r, echo=TRUE}
library(readr)
library(dplyr)
library(ggplot2)
library(corrplot)
library(lubridate)
library(tidyr)
library(tidyverse)
library(GGally)

Full_Database<-read.csv("Full_database_VarCatHeaders.csv", header=TRUE)


Full_Database

```

```         
```

## Clean the Data

The fields are prefixed by a category label, derived from the codebook:

```{r}

tvp_dd<-read.csv("TVP_DataDictionary.csv")
str(tvp_dd)

tvp_dd$CAT_ABBREV<-paste(tvp_dd$ABBREV, tvp_dd$CATEGORY, sep="_")
str(tvp_dd)

tvp_dd %>%
  group_by(CAT_ABBREV) %>%
  summarize("Num_Vars"=n())

head(tvp_dd)

```

### Convert Categoricals to Factors

For any machine learning experiment, the categorical variables need to be converted to factors. The variables which be factors have already been identified manually based on the Codebook.

```{r}
# Which fields in the Full Database are factors?
dd_factors<-subset(tvp_dd, TYPE=="Factor" & TABLE=="Full")
#dd_factors
factors_list<-dd_factors$VARIABLE

Full_Database<-Full_Database %>%
  mutate_at(factors_list, as.factor)




#str(Full_Database)
```

### Eliminating Unnecessary Features

For the purposes of cluster analysis of location, many of the factors included in the Full_Database are not relevant. Whole categories we *excluded* include:

-   TACE (Trauma, Abuse, and Childhood Experience)

-   V (Victims)

-   SC (Signs of Crisis)

Whole categories of fields we *will* include:

-   B (Basic)

-   D (Date)

-   L (Location)

-   COMM (Community)

-   F (Firearms)

-   W (Weapons)

-   GM (Grievance and Motivation) - association with regions or states

Then there are selected variables from the remaining categories which *may* be relevant or useful in the modeling of locations:

-   OB_Gender

-   OB_Race

-   OB_Age

-   CV_CriminalRecord (when compared with state firearms laws)

-   HMH_PriorHospitalization (again, for analysis against state firearms laws)

-   SOC_Leakage

-   RC_WhoKilledShooterOnScene - to be combined with L_ArmedPersonOnScene

The remaining variables from these categories are not needed.

```{r}
location_df<-Full_Database %>%
  select(starts_with("B_") |
  starts_with("D_") | 
  starts_with("L_") |
  starts_with("W_") |
  starts_with("COMM_") |
  starts_with("F_") |
  starts_with("GM_") |
  starts_with("OB_Gender") |
  starts_with("OB_Age") |
  starts_with("OB_Race") |
  starts_with("CV_CriminalRecord") |
  starts_with("HMH_PriorHospitalization") |
  starts_with("SOC_Leakage") |
  starts_with("RC_WhoKilledShooterOnScene"))


head(location_df)
```

### Evaluating missing values

Then I'm going to find the features with the fewest missing variables. I'm going to try the *vis_miss()* function of the **visdat** library.

```{r}
library(visdat)
location_df[] <- lapply(location_df, function(x) {
    is.na(levels(x)) <- levels(x) == "NA"
    x
})

vis_miss(location_df)
```

It looks like there are only a few fields for which there are substantially missing data, but it's hard to read the column names. Better to examine the distribution of usage by variable.

```{r}

library(tidyverse)
# solution found https://stackoverflow.com/questions/44485428/store-output-of-sapply-into-a-data-frame

#count the number missing
location_df_missing<-data.frame(sapply(location_df, function(x) sum(is.na(x))))

#bind to 
location_df_missing<- cbind(variable = row.names(location_df_missing), location_df_missing)

location_df_missing<-location_df_missing %>% 
  rename(num_missing=sapply.location_df..function.x..sum.is.na.x...)
location_df_missing<-location_df_missing %>% 
  mutate(location_df_missing, pct_missing = as.numeric(location_df_missing$num_missing)/ as.numeric(195))
location_df_missing<-location_df_missing[order(-location_df_missing$num_missing), ]
location_df_missing
barplot(t(as.matrix(location_df_missing$pct_missing)), beside=TRUE)
```

Based on the table of missing data, the fields with significant amounts of missing data are essentially the long-text fields, which I'm not going to be using anyhow. So, I'll just remove them from the dataframe.

```{r}
location_df<-location_df %>%
  select(-starts_with("L_Other")) %>%
  select(-starts_with("W_SpecifyOther")) %>%
  select(-starts_with("SOC_LeakageHow")) %>%
  select(-starts_with("SOC_LeakageWho")) %>%
  select(-starts_with("SOC_LeakageSpec"))
head(location_df)
vis_miss(location_df)
```

## Join / Merge the data

We want to join the data on state and year to gain insights into state population and the state firearms laws in a given time period. The population estimates were downloaded from <https://github.com/JoshData/historical-state-population-csv>, which were originally from the U.S. Census. The firearms laws data was extracted from the [State Firearms Law Database](https://www.statefirearmlaws.org/resources), originally created for a research project. The key data point extracted from this set is the total number of *restrictive* firearms laws for the state and effective year of the mass shooting event. The main limitation of this data is that it only covers the period from 1991 through 2021.

Both of these data will be merged with the Full_Dataset based on both State and Year. NOTE: the State Firearms Law Database has already been matched against the state and year prior to being added to this R Project. Hence, this extracted dataset will be matched based on the case number.EDA

```{r}
historical_state_population_by_year<-read.csv("historical_state_population_by_year.csv", header=TRUE)

# KH: read the State Firearms Law table that provides sums of restrictive gun laws for that state in the year of that event.
sfl_sum <- read.csv("SFL_Sum_Year_by_MSDB_Case.csv", header=TRUE)

# Merge data and keep all rows from Full_Database
location_df <- location_df %>%
    left_join(historical_state_population_by_year, by=c("L_State" = "PopulationState", "D_Year" = "StatePopulationYear"))

# KH: joined with the SFL dataset, adding SLF_Sum_Year column
location_df <- location_df %>%
   left_join(sfl_sum, by="B_CaseNum")

location_df
```

### Assign Labels to Factor Levels

These are derived from the Codebook.

```{r}
location_df$L_Region<- ordered(location_df$L_Region, 
                                 levels=c(0,1,2,3),
                                 labels=c("South","Midwest","Northeast","West"))
location_df$L_Urban_Suburban_Rural<- ordered(location_df$L_Urban_Suburban_Rural, 
                                 levels=c(0,1,2),
                                 labels=c("Urban","Suburban","Rural"))
location_df$L_Location<- ordered(location_df$L_Location, 
                                 levels=c(0,1,2,3,4,5,6,7,8,9,10),
                                 labels=c("K-12 School","College/Univ","Govt Bldg","House of Worship","Retail","Rest/Bar/NC","Office","Residence","Outdoors","Warehouse","Post Office"))

location_df$L_SpecifyArmedPerson<- ordered(location_df$L_SpecifyArmedPerson, 
                                 levels=c(0,1,2),
                                 labels=c("N/A","Law Enforcement","Civilian"))
location_df$OB_Gender<- ordered(location_df$OB_Gender, 
                                 levels=c(0,1,3,4),
                                 labels=c("Male","Female","Non-Binary","Transgender"))
location_df$OB_Race<- ordered(location_df$OB_Race, 
                                 levels=c(0,1,2,3,4,5),
                                 labels=c("White","Black","Latinx","Asian","Middle Eastern","Native American"))
location_df$W_FirearmProficiency<-ordered(location_df$W_FirearmProficiency,
                                          levels=c(0,1,2,3),
                                          labels=c("No evidence","Some experience","More experienced","Very experienced"))
location_df$GM_KnownPrejudices<-ordered(location_df$GM_KnownPrejudices,
                                          levels=c(0,1,2,3,4),
                                          labels=c("No evidence","Racism","Misogyny","Homophobia","Religious Hatred"))
location_df$GM_Motive_Racism_Xenophobia<-ordered(location_df$GM_Motive_Racism_Xenophobia,
                                          levels=c(0,1,2),
                                          labels=c("No evidence","Targeting people of color","Targeting white people"))
location_df$GM_Motive_ReligiousHate<-ordered(location_df$GM_Motive_ReligiousHate,
                                          levels=c(0,1,2,3,4),
                                          labels=c("No evidence","Antisemitism","Islamaphobia","Angry with Christianity/God","Both Antisemitism and Islamaphobia"))
location_df$GM_Motive_Other<- ordered(location_df$GM_Motive_Other,
                                      levels=c(0,1,2),
                                      labels=c("No evidence","Yes","Generalized anger"))
location_df$GM_RoleofPsychosisintheShooting<- ordered(location_df$GM_RoleofPsychosisintheShooting,
                                      levels=c(0,1,2,3),
                                      labels=c("No evidence","Minor","Moderate","Major"))
location_df$GM_Motive_Other<- ordered(location_df$GM_Motive_Other,
                                      levels=c(0,1,2),
                                      labels=c("No evidence","Yes","Generalized anger"))
location_df$RC_WhoKilledShooterOnScene<- ordered(location_df$RC_WhoKilledShooterOnScene,
                                      levels=c(0,1,2,3,4),
                                      labels=c("Alive","Killed self","Killed by police","Killed by school resource officer","Killed by armed civilian"))
                                          

```

## EDA of Location Data

The exploratory data analyses we will conduct will be primarily for targeting the features to include in our model. First we need to establish the distribution of each of the columns. For this, we used the library, [DataExplorer](https://cran.r-project.org/web/packages/DataExplorer/index.html) and [GGally](https://cran.r-project.org/web/packages/GGally/index.html). The introduce() function shows the distribution of discrete vs. continuous data, as well as how complete our data is. The results show that nearly 85% of our columns are categorical and that there are few missing data now.

```{r}
library(DataExplorer)
library(GGally)

introduce(location_df)
plot_intro(location_df)
```

### Continuous Data

Since there are only 15% of columns which are continuous, let's look at these first and get them out of the way. So, which are the interval fields?

```{r}
matchingfields<-sapply(location_df, function(x) is(x, "numeric"))
matchingfieldsnames<-names(location_df)[matchingfields]
print(matchingfieldsnames)
```

So, D_Year could be analyzed as continuous or categorical; OB_Age and StatePopulationNumber should be analyzed as numeric. The others, not so much. So we'll first convert those fields to text. Then we'll look at descriptive data and then at the histograms of the distributions. We'll use Basic R's summary() function for the former, and DataExplorer's plot_histogram() for the latter. The latitudes and longitudes should remain continuous, but we don't need to analyze these as such.

```{r}
location_df$B_CaseNum<-as.factor(location_df$B_CaseNum) 
location_df$L_ZipCode<-as.factor(location_df$L_ZipCode) 
location_df$L_SpecifyArmedPerson<-as.character(location_df$L_SpecifyArmedPerson)

summary(Filter(is.numeric, location_df))

plot_histogram(location_df)
```

The key data, D_Year, OB_Age, and StatePopulationNumber, are all heavily skewed (right, left, and then right again). This means that frequency appears to have increased over time, that more mass shooters appear to be younger than older, and more mass shooting events appear to occur in states with smaller populations than in larger. This latter observation may be worth pursuing.

Let's see how we can categorize these variables by looking at their distribution by percentile.

### Categorical Data

Now, let's examine the categorical data using DataExplorer's plot_barchart(). I'll analyze these by category, which, using the variable-naming structure, is fairly easy to do.

```{r}
library(dplyr)
library(ggplot2)
is.fact<-sapply(location_df, is.factor)
location_df_factors<-location_df[, is.fact]
ob_df<- location_df_factors %>%
  select(starts_with("OB_") | starts_with("SOC_") | starts_with("HMH_") | starts_with("CV_"))
gm_df<- location_df_factors %>%
  select(starts_with("GM_"))
loc_df<- location_df_factors %>%
  select(starts_with("L_") | starts_with("SFL_") | starts_with("RC_"))

fw_df<- location_df_factors %>%
  select(starts_with("F_") | starts_with("W_"))
plot_bar(ob_df)
plot_bar(gm_df)
plot_bar(loc_df)
plot_bar(fw_df)


```

To make cluster analyses work well, we will need to reduce the number of levels, as well as the number of features. Towards that end, it seems that these adjustments make the most sense:

-   We'll ignore gender - it is too dominated by males to take females or non-binary into consideration.

-   While I'm reluctant to make race binary, given the predominance of White offenders, it makes sense to use White and Non-White. We may want segment out Black and Latinx, leaving an Other category.

-   At first glance, there doesn't seem to be enough of those with prior hospitalization (psychiatric, that is), but before I shed this feature, I want to see if there is a correlation with other variables.

-   Make all of the GM variables (Grievance and Motivation) binary.

-   The Location variable (that is, the type of location) needs to be reduced, but I'm not sure how to combine the categories. Here's one option which reduces 10 categories to 5:

    -   Education (School and College/University)

    -   Public Non-Govt (Retail and Rest/Bar/NC and Outdoors and House of Worship)

    -   Govt (Govt and Post Office)

    -   Workplace (Office and Warehouse)

    -   Residence

-   The Metro_MicroStatisticalArea doesn't have enough variety to warrant inclusion.

-   While there aren't very many with armed personnel on scene, there is enough to warrant inclusion, especially when combined with shooter outcome.

So, let's make these changes.

```{r}
location_df<- location_df %>%
  
# make 2 new variables for race - one binary and one with 4 categories 
  mutate(OB_White=case_when(OB_Race==1 ~ "White",
                                  TRUE ~ "Non-White"), 
         OB_Race_multi=case_when(OB_Race==0 ~ "White",
                                  OB_Race==1 ~ "Black",
                                  OB_Race==2 ~ "Latinx",
                                  TRUE ~ "Other"),
# make new binary variables for GM_KnownPrejudices and GM_Motive_ReligousHate   
        GM_Prejudiced=case_when(GM_KnownPrejudices==0 ~ 0,
                                TRUE ~ 1),
        GM_Motive_ReligiousHate_Binary=case_when(GM_Motive_ReligiousHate==0 ~ 0,
                                                 TRUE ~ 1),

# make new L_Location variable that has only 5 categories   
        L_Location_Multi=case_when(L_Location<2 ~ "School/College",
                                   (L_Location==2 | L_Location==10) ~ "Govt/PO",
                                   (L_Location==3 | L_Location==4 | L_Location==5 | L_Location==8) ~ "Public Non-Govt",
                                   (L_Location==6 | L_Location==9) ~ "Workplace",
                                   TRUE ~ "Residence"
        ),
# remove the fields from which these were derived
      OB_Race=NULL,
      GM_KnownPrejudices=NULL,
      GM_Motive_ReligiousHate=NULL,
      L_Location=NULL,
                                   
# remove OB_Gender and L_Metro_MicroStatisticalArea
      OB_Gender=NULL,
        L_Metro_MicroStatisticalAreaType=NULL
      
  )
head(location_df)
# remove other fields not necessary for analysis, like names and case numbers and addresses
location_df<- location_df %>%
  mutate(B_CaseNum=NULL,
         B_ShooterLastName=NULL,
         B_ShooterFirstName=NULL,
         D_FullDate=NULL,
         L_StreetNumber=NULL,
         L_StreetName=NULL,
         L_City=NULL,
         L_County=NULL,
         L_ZipCode=NULL,
         L_Latitude=NULL,
         L_Longitude=NULL,
         L_StateCode=NULL
         )
head(location_df)
```

### 

#### Date Data

Visualizations of date data works better if shown in date or time order. Doing so requires modifications to the data so that the order of the factors is retained. To do this, we convert the Day of Week and the Day of Month and the Month variables into two-digit characters with leading zeros in order to force alphabetical ordering. Then we display bar charts of these derived columns.\

```{r}

# put the date fields in time-order
location_df$D_DayofWeek<-factor(location_df$D_DayofWeek, 
                                levels=c("Monday","Tuesday","Wednesday","Thursday","Friday","Saturday","Sunday"))

location_df<-location_df %>% 
  mutate(D_DOW_Num= 
         case_when(D_DayofWeek=="Monday" ~ "1",
            D_DayofWeek=="Tuesday" ~ "2",
            D_DayofWeek=="Wednesday" ~"3",
            D_DayofWeek=="Thursday" ~"4",
            D_DayofWeek=="Friday" ~"5",
            D_DayofWeek=="Saturday" ~ "6",
            D_DayofWeek=="Sunday" ~"7"))

location_df$D_DOW_Num<-factor(location_df$D_DOW_Num, labels(c("Monday","Tuesday","Wednesday","Thursday","Friday","Saturday","Sunday")))
location_df_factors$D_DOW_Num<-as.factor(location_df$D_DOW_Num)
location_df<- location_df %>%
  mutate(D_DOM_Num=
           case_when(nchar(as.character(D_Day))<2 ~ paste("0",D_Day, sep=""),
                     TRUE ~ D_Day)
  )
location_df_factors$D_DOM_Num<-as.factor(location_df$D_DOM_Num)

date_df<- location_df_factors %>%
  select(starts_with("D_"))

#using ggplot for the date fields because order of values is important
ggplot(date_df, aes(x=D_DOW_Num, label=D_DOW_Num))+
  geom_bar()
ggplot(date_df, aes(x=D_DOM_Num, label=D_DOM_Num))+
  geom_bar()
ggplot(date_df, aes(x=D_Month, label=D_Month))+
  geom_bar()
```

The trend by day of the week appears more skewed to the early part of the week, so perhaps making this binary would be useful. The distribution by day of the month appears multi-modal, with a curve starting at the beginning of the month and skewing right, followed by another right-skewed curve starting on the 14th (the mode of the entire set). Again, making this binary could be more useful as a feature than 31 individual factors. As far as the months of the year go, March is a key month, and there is an upward trend from September though December. Summer months are surprisingly stable and moderate.

Based on previous literature, there is an association between location type (notably schools and workplaces) and months and days of week (Schildkraut, et al. 2022). I think making Months binary into school year and not school year (Oct-Mar, Apr-Sep) would be useful.

```{r}
location_df <- location_df %>%
  mutate(
    D_EarlyWeek=case_when(as.integer(D_DOW_Num) < 4 ~ 1,
            TRUE ~ 0), 
        D_SchoolYear= case_when(
          (as.integer(D_Month)>9 | as.integer(D_Month)<4) ~ 1,
            TRUE ~ 0),
          D_EarlyMonth=case_when(
            as.integer(D_DOM_Num)<14 ~ 1,
            TRUE ~ 0),
    #remove other date fields from analysis
    D_DayofWeek=NULL,
    D_Day=NULL,
    D_Month=NULL,
    D_DOW_Num=NULL,
    D_DOM_Num=NULL
  )
date_df$D_EarlyWeek<-location_df$D_EarlyWeek
date_df$D_SchoolYear<-location_df$D_SchoolYear
date_df$D_EarlyMonth<-location_df$D_EarlyMonth
head(date_df)
head(location_df)
```

### Correlational Analysis

## Data Modeling

```{r}

```

## Cluster Analysis

```{r}

```

## Boot strapping

```{r}

```

## Data Reporting & Communication Results

## 
